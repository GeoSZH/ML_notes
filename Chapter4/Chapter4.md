## 决策树
决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别（即叶结点）中。一般而言，决策树的生成包含了特征选择、树的构造、树的剪枝三个过程。

首先是一段决策树的伪代码，我们可以看出来决策树的生成是一个递归过程，而回顾决策树的思想，在遇到未知样本时，决策树的基本流程就是简单且只管的“分而治之”策略。

![image](https://user-images.githubusercontent.com/88269254/170398654-ab1153ec-aaa0-4f33-93dd-66e8c2e551bf.png)

在决策树基本算法中，有三种情形会导致递归返回: 

1. **当前结点包含的样本全属于同一类别，无需划分**; 
2. **当前属性集为空，或是所有样本在所有属性上取值相同，无法划分**; 
3. **当前结点包含的样本集合为空，不能划分**。

在第2种情形下，我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;在第3种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不同:情形2是在利用当前结点的**后验分布**，而情形3则是把父结点的样本分布作为当前结点的**先验分布**。

### 特征选择
由伪代码可看出决策树学习的关键是第8行，即如何选择最优划分属性一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的"**纯度**" (purity) 越来越高。

下面介绍用来特征选择的几种方法，同样也可以对应于决策树有哪些启发函数等。

#### 信息增益（ID3）
![image](https://user-images.githubusercontent.com/88269254/170403271-ae26930f-722d-4676-9cf1-290560653237.png)
这里是先放了信息熵的概念，这个信息熵个人理解就是图片中所说的"**纯度**"，当一个集合中两类样本五五开（或者多类样品全部均分）的时候信息熵是最大的，代表信息是"**纯度**"低，混乱的（个人理解），新来一个样本我们无法确定其类别，反之，当集合只有一类样本的时候，明显，此时信息熵为0，"**纯度**"很高，来一个新样本，我们几乎可以确认它就是这个类别的。

再来介绍信息增益，在信息论中信息增益也称为**互信息**，其表示**已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度**。所以在这里，这个公式可以理解为在属性a 的取值已知后，样本类别这个随机变量的不确定性减小的程度。若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，也即为书上所说的“纯度提升”越大。

![image](https://user-images.githubusercontent.com/88269254/170404000-4531680b-b816-44ef-b849-2129ce3abc23.png)

我们知道大概是这么计算的就可以了，至于具体的例子这里就不放出来了，书上有计算的实例，结合定义很好理解，这里只给出逻辑上的思路。

#### 增益率（C4.5）
![image](https://user-images.githubusercontent.com/88269254/170405684-4cbf1b8f-0c82-4c2b-9965-86afe7c4d6e5.png)

需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此， C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

#### 基尼指数（CART决策树）
![image](https://user-images.githubusercontent.com/88269254/170405903-57c616a4-1531-4331-b1b0-9aac0fbeffbe.png)

### 剪枝处理
剪枝(pruning)是决策树学习算法对付"过拟合"的主要手段.在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得"太好"了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动去掉一些分支来降低过拟合的风险.

决策树剪枝的基本策略有"预剪枝" (prepruning)和"后剪枝"(post-pruning)。预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点（采用留出法预留一部分作为验证机以判断决策树泛化性能是否提升）。

#### 预剪枝
![image](https://user-images.githubusercontent.com/88269254/170406875-2e3bb471-1b43-48e4-b13a-cdeff2d6c18b.png)

对比图4.6 和图4.5 可看出，于预页剪枝使得决策树的很多分支都没有"展开"，这不仅降低了过拟合的风险，还显著减少了决策树训练时间开销和测试时间开销.但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高;预剪枝基于"贪心"本质禁止这些分支展开，给预剪枝决策树带来了欠拟含的风险。

#### 后剪枝
![image](https://user-images.githubusercontent.com/88269254/170407098-63dc232c-a003-475a-9187-72a1538a50d3.png)

对比图4.7和图4.6 可看出，后剪枝决策树通常比预剪枝决策树保留了更多的分支. 一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树.但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

**启发函数的实例和剪枝的详细介绍会在另一份文档中详细给出。**

### 连续与缺失值
#### 连续值处理
到目前为止我们仅讨论了基于离散属性来生成决策树. 现实学习任务中常会遇到连续属性，有必要讨论如何在决策树学习中使用连续属性.由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分.此时，连续属性离散化技术可派上用场. 最简单的策略是采用二分法(bi-partition)对连续属性进行处理，这正是C4.5 决策树算法中采用的机制。
![image](https://user-images.githubusercontent.com/88269254/170408819-5def5e7f-13d2-450c-afac-a158b9f37b8b.png)
![image](https://user-images.githubusercontent.com/88269254/170408872-c20bd45d-cf7d-4742-93a1-3b9d9d8221c6.png)

实例这里暂未给出，可以书中查阅，需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性.例如在父结点上使用了"密度≤0.381" ，不会禁止在子结点上使用"密度≤0.294"。
#### 缺失值处理
