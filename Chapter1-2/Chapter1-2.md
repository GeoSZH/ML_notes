本篇笔记主要是用来查漏补缺，基础的知识可能一些并没有详细解释，主要是用于记录一些较为容易遗忘的知识点。

### Chapter1 绪论

#### 1.1 假设空间
从数据中学得模型所对应的关于数据的某种潜在规律，称之为**假设**。

这种潜在规律自身，则称之为**真相**或**真实**。学习过程就是为了找出或逼近真相。而我们可以把学习过程看作是一个在所有假设组成的空间中进行搜索的过程，这个空间即是**假设空间**，搜索目标则是找到与训练集匹配的假设，即能在训练集中**判断正确**的假设。

#### 1.2 归纳偏好
![image](https://user-images.githubusercontent.com/88269254/169642252-e453dc14-cc28-4b64-aa80-38873b607c80.png)
![image](https://user-images.githubusercontent.com/88269254/169642314-5944368f-718d-4b99-a13b-49bb8a3d057c.png)

我们根据表1.1提供的训练集可以得出表格下方能够使训练集一致的三个假设（其中的'* '代表**通配符**）,那么基于现有的样本无法断定上述的三个假设中哪一个“**更好**”，但是对于一个具体的学习算法而言，它必须产生**一个**模型。这时，学习算法本身的“偏好”就会起到关键作用。例如，若我们的算法喜欢"**尽可能特殊**"的模型，则它会选择"(色泽= * )^(根蒂=蜷缩)^(敲声=浊响) = 好瓜" ;但若我们的算法喜欢"**尽可能一般**"的模型，并且由于某种原因它更"相信"根蒂，则它会选择"(色泽= * )^(根蒂=蜷缩)^(敲声= * ) = 好瓜" 。机器学习算法在学习过程中对某种类型假设的偏好，称为"**归纳偏好**" (inductive bias) ,或简称为"**偏好**"。一个更直观的图如下，其中A，B可以认为是不同学习算法的可能偏好：

![image](https://user-images.githubusercontent.com/88269254/169643108-32f7c961-ea5c-4d29-8109-7a59cb94fd7d.png)

任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上"等效"的假设所迷惑，而**无法产生确定的学习结果**。那么归纳偏好在我们使用算法过程中有什么影响呢？事实上，归纳偏好对应了学习算法本身所做出的关于"什么样的模型更好"的假设.在具体的现实问题中，这个假设是否成立，即**算法的归纳偏好是否与问题本身匹配**，大多数时候直接决定了算法能否取得好的性能。

对于一个学习算法A，若它在**某些问题**上比学习算法B好，则必然存在**另一些问题**，在那里B比A好。有趣的是，这个结论对任何算法均成立。也就是说哪怕把“**随机胡猜**”和最近大火的"**LightGBM**"。接下来是一些证明过程，摘自南瓜书和西瓜书（图片格式，未用LaTeX）。

![image](https://user-images.githubusercontent.com/88269254/169643675-547b5cbe-d089-4ff6-8692-eb74beec35a9.png)

![image](https://user-images.githubusercontent.com/88269254/169643547-ff5a2ab1-286b-40a3-b344-09fe326a47c6.png)

![image](https://user-images.githubusercontent.com/88269254/169643733-e28474a5-1b73-4378-b757-c8ece2d67655.png)


### Chapter2 模型评估与选择
#### 2.1 经验误差与过拟合
这里不详细介绍错误率，精度，误差等等概念。想主要总结一下过拟合和欠拟合出现的**原因以及解决方法**：

首先我们先看一张西瓜书里对过拟合和欠拟合介绍的一张图

![image](https://user-images.githubusercontent.com/88269254/169643980-c15f4a7c-30e1-477b-90f8-36ff4959f61c.png)

有很多中因素可能导致过拟合，其中比较经常出现的就是：

（1）训练数据较少

（2）模型复杂度较高

究其根本，就是由于**学习能力太过强大**，以至于把**不太一般的特征**都学到了。解决方案也可根据上面的原因而对症下药，比如说，**增加训练数据，降低模型复杂度，添加或者调大正则化系数，或是采用集成方法**来缓解过拟合带来的影响。

也有很多因素可以导致欠拟合，不过欠拟合相对于过拟合来说比较容易克服：

（1）特征不足

（2）特征与标签的关联性不强

这个根本原因跟过拟合相反，即**学习能力不足**，而我们要做的对症下药的操作就是，**添加新特征，使模型更复杂一些（对应决策树则是扩展分支，神经网络增加层数或者增加训练轮数），或者减小正则化系数**

#### 2.2 评估方法
其中包括留出法，交叉验证法，自助法等，这里只做简单介绍：

##### 2.2.1 留出法

将数据集D划分未两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=Ø，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练集和测试集的划分要尽可能保持数据分布的一致性，以便面由于分布的差异引入额外的偏差，常见的作法是采用**分层抽样**。同时，由于划分的随机性，单词的留出法结果往往不够稳定，一般要采用若干次随即划分，重复实验取平均值的做法。

##### 2.2.2 交叉验证法

将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪...∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用**分层抽样**的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“**k折交叉验证**”。与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为**p次k折交叉验证**，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“**留一法（LeaveOneOut）**”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。

##### 2.2.3 自助法

自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D'。可以得知在m次采样中，样本始终不被采到的概率取极限为两个重要极限之一，结果**1/e**。这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D'中，于是可以将D'作为训练集，D-D'作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。

#### 2.3性能度量

**性能度量**（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。

##### 2.3.1 回归任务性能度量

针对回归任务中连续值预测的问题，最常见的性能度量是“**均方误差（MSE）**”，很多经典的算法都是采用了MSE作为评价函数。这里不过多介绍。

##### 2.3.2 分类任务性能度量

这里面有很多，常见的比如说错误率和精度，这里不详细介绍了，主要介绍一下以下几个概念：**查全率（Precision）**，**查准率（recall）**，和**F1**，并引出**P-R曲线**和**ROC曲线**。

查全率**P=TP/(TP+FP)**

查准率**R=TP/(TP+FN)**

F1则是P和R的调和平均**1/F=1/2 * (1/P + 1/R)**,计算得**F1=2PR/(P+R)**

为什么要引入F1这个概念呢，因为我们在具体的应用中对P和R的重视程度有所不同，我们可能更在乎其中一个指标，于是我们可以使用**加权调和平均**，其中β>1时P影响更大，β<1时R影响更大：

![image](https://user-images.githubusercontent.com/88269254/169698537-d6c42088-53cd-4dce-8ab8-96479e6a74ef.png)

而P-R曲线直观地显示了学习器在样本总体上的查全率和查准率，我们在进行不同的学习器之间的比较时，若一个学习器的P-R曲线被另一个完全“**包住**”，则可以说前者的性能不如后者。如图，A和B在这个例子中都是比C更好的学习器，但是我们如果还要比较AB的话，这时一个比较合理的判断依据就是比较**P-R曲线下面积的大小**，它在一定程度上表征了学习器在查准率和查全率上取得相对"双高"的比例。

![image](https://user-images.githubusercontent.com/88269254/169698791-834f1b3c-cc4d-4683-9afd-8a6fe6392483.png)

但是往往曲线下面积不容易估算，所以有人提出了一个“**平衡点**”，也就是**查准率=查全率**时的取值，图中也有标记，可以看出在这个例子的比较中，A学习器是优于B的。

PS：我们虽然知道了P-R曲线的意义，但是怎么把它画出来呢？

我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为"**最可能**"是正例的样本排在最后的则是学习器认为"**最不可能**"是正例的样本.按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率以查准率为纵轴、查全率为横轴作图，就得到了P-R曲线。

最后介绍一下**ROC**与**AUC**，ROC曲线的横纵坐标是两个不同的值，分别为**真正例率**，真正例率=TP/(FP+FN)和**假正例率**，假正例率=FP/(TN+FP)。

![image](https://user-images.githubusercontent.com/88269254/169699287-08d944ed-b67b-418d-b0f8-b242967eff29.png)

那么ROC曲线图又是怎么画出来的呢？

给定m<sup> + </sup>个正例和m<sup> - </sup>个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标(0,0) 处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为(x,y)，当前若为真正例，则对应标记点的坐标为(x,y+1/m<sup> + </sup>) ;当前若为假正例，则对应标记点的坐标为(x+1/m<sup> - </sup>,y) ，然后用线段连接相邻点即得。

但是限时任务中通常都是有限个测试样例来绘制ROC图，此时仅能获得一些坐标对，也就是如图右所示。进行学习器的比较时， 与P-R图相似， 若一个学习器的ROC曲线被另一个学习器的曲线完全"**包住**"，则可断言后者的性能优于前者;若两个学习器的ROC曲线发生交叉，则难以-般性地断言两者孰优孰劣. 此时如果一定要进行比较， 则较为合理的判据是比较ROC 曲线下的面积，即AUC (Area Under ROC Curve)，AUC的公式为：
![image](https://user-images.githubusercontent.com/88269254/169701125-deb62470-30ec-4c1e-8e54-1fe10978018e.png)

以上是西瓜书的相关介绍，这里还有更加详细的对于公式的解释，参考自南瓜书：

![image](https://user-images.githubusercontent.com/88269254/169701167-01386a2d-11fb-4601-8346-c0b3650b4e0a.png)

至于为什么是AUC，AUC背后的含义是什么可以参考西瓜书以下的这段话，如果感兴趣公式的话，西瓜书附图后也附上了该**损失**的公式推导：

![image](https://user-images.githubusercontent.com/88269254/169701308-53c8e536-8937-47db-9f8d-55416c35d9ec.png)
![image](https://user-images.githubusercontent.com/88269254/169701319-7a7db587-fdfb-48c9-9794-f4e642d6a9c6.png)

#### 2.4 比较检验
暂未有深刻理解，这里就不班门弄斧了，略过以后补充。

#### 2.5 偏差与方差
偏差-方差分解是解释学习器泛化性能的重要工具。下面介绍偏差、方差、噪声的含义和依次的公式：偏差度量了学习算法的额期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力;方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响;噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的F 界，即刻画了学习问题本身的难度。

![image](https://user-images.githubusercontent.com/88269254/169701983-243835aa-09c7-4d0a-b9bc-f37e00b06fc7.png)
![image](https://user-images.githubusercontent.com/88269254/169701987-1bbb0f3d-3011-4e55-a2d8-6419a483355e.png)

![image](https://user-images.githubusercontent.com/88269254/169702045-559f429d-94f3-4c86-acc8-a28b55d7288d.png)


- **期望泛化误差=方差+偏差**
- **偏差刻画学习器的拟合能力**
- **方差体现学习器的稳定性**

![image](https://user-images.githubusercontent.com/88269254/169702074-0ac1a260-e002-4f43-ba9a-9941f981ad5a.png)
